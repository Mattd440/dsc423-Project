---
title: "GraduationRateAnalysis"
author: "MattDiederick"
date: "February 28, 2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "Project-423"
author: "MattDiederick"
date: "February 28, 2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Librarys - Load File - clean up data
```{r}
  library(ggplot2)
  library(dplyr)
  library(tidyr)
  library(car)
  library(psych)
  library(leaps)
  library(caret)
  library(QuantPsyc)

  # Load File
  df = read.delim('Admission.csv', header = TRUE, sep=',')

  #Drop Serial.No Column
  grad_df = df[,-c(1)]
  
  #SetUp For later Use
  cormatrix = as.data.frame(round(cor(grad_df), 3))
  cormatrix = cormatrix %>% mutate(var1 =rownames(cormatrix))
  
  #Cast Qualitative Variables to Factors
 #grad_df$University.Rating = as.factor(grad_df$University.Rating)
  grad_df$Research = as.factor(grad_df$Research)
  #grad_df$SOP = as.factor(grad_df$SOP)
  #grad_df$LOR = as.factor(grad_df$LOR)
  
  grad_df$GRE.Score = scale(grad_df$GRE.Score, scale=TRUE, center=TRUE)
  grad_df$TOEFL.Score = scale(grad_df$TOEFL.Score, scale=TRUE, center=TRUE)
  head(grad_df,10)
  
  inds = sample(1:nrow(grad_df), .8*nrow(grad_df))
  train_df = grad_df[inds,]
  test_df = grad_df[-inds,]
  
  
```
 
 
# Visualize Data

```{r}

ggplot(data=grad_df, aes(x=Chance.of.Admit))+
  geom_histogram(bins=20)+
  ggtitle('Distribution of Admittance Chance')

ggplot(data=grad_df, aes(x=GRE.Score, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE) +
  ggtitle("Admission by GRE Score")

ggplot(data=grad_df, aes(x=TOEFL.Score, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)+
  ggtitle("Admission by TOEFL Score")

ggplot(data=grad_df, aes(x=SOP, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)+
  ggtitle("Admission by SOP Strength")

ggplot(data=grad_df, aes(x=LOR, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)+
  ggtitle("Admission by GRE Score")+
  ggtitle("Admission by LOR Strength")

ggplot(data=grad_df, aes(x=CGPA, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)+
  ggtitle("Admission by CPGA")

ggplot(data=grad_df, aes(x=University.Rating, y=Chance.of.Admit, col=Research)) +
  geom_boxplot()+
  ggtitle("Admission by University Ratings")



```

# View Correlations 


```{r}

cleanmatrix = gather(data=cormatrix, key='var2', value='val', -var1)

cormatrix

ggplot(cleanmatrix, aes(x=var1, y=var2, fill=val)) + 
 geom_tile(colour='white', size=.25)+
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))+
  scale_colour_gradientn(colours=terrain.colors(10))


```


#define function to change ODDS back to PROBABilty
#define function to plot residuals
```{r}
sigmoid = function(x){
  return (1/(1+exp(-x)))
}


plotResiduals = function(data,model){
  ggplot(data=data,aes(x=sigmoid(fitted(model)), y=rstudent(model))) +
    geom_point() +
    geom_hline(yintercept=0, col='red') +
    ggtitle('Fitted values vs standard residuals')
}

plotNormalResidual = function(model){
  qqnorm(rstandard(model))
  qqline(rstandard(model), col='red')
}


```

# FIT FULL MODEL 
* Here we transform Chance.of.Admit using log(x/1-x) that changes it from a probability to odds. We then use the sigmoid function above to change our fitted values which are odds back to probability. This ensure our predictions stay between 0 and 1 since we our predicting probability. These functions our used in logistic regression , but we are predicting probability and not a binary variable so we use the linear model with transformations on the response variable.  
```{r}

full_model = lm(data=train_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit)) ~ . )
summary(full_model)
vif(full_model)

plotResiduals(train_df, full_model)
plotNormalResidual(model)


```



# Apply Variable Selection


```{r}
base = lm(data=train_df, formula = Chance.of.Admit ~ 1)
#bward = step(full_model, direction='backward', trace=TRUE)
fward = step(base, scope=list(upper=full_model, lower=~1), direction='forward', trace=TRUE)

```



# Fit Model With Selected Variables

```{r}

model2 = lm(data=train_df, formula = log(Chance.of.Admit/(1-Chance.of.Admit)) ~CGPA + GRE.Score + LOR + Research+TOEFL.Score +University.Rating)
summary(model2)
```



# Fit Model With ALL Interaction Terms

```{r}

model3 = lm(data=train_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit))~(CGPA+ GRE.Score + LOR + Research+ University.Rating)^2)
summary(model3)

```



#Fit Model With Research Interaction

```{r}

model4 = lm(data=train_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score+ Research:TOEFL.Score + Research:CGPA + Research:GRE.Score +Research:LOR+Research:University.Rating)
summary(model4)
```

# Remove Insignificant Features
```{r}
model5= lm(data=train_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score + Research:CGPA  +Research:University.Rating)
summary(model5)
```

#PLOT RESIDUALS
```{r}
plotResiduals(train_df, model5)
plotNormalResidual(model5)

```

#CHECK FOR OUTLIERS
```{r}
infl = influence.measures(model5)
summary(infl)
print('*******COOKS*********')
cooks = cooks.distance(model5)
head(sort(desc(cooks)),10)
print('******HAT**********')
hats = hatvalues(model5)
head(sort(desc(hats)),10)
print('*******RSTUDENT*********')
rs = rstudent(model5)
head(sort(desc(abs(rs))), 10)

ggplot(data=train_df, aes(x=hatvalues(model5), y=rstudent(model5)))+
  geom_point() + 
  geom_hline(yintercept = 3, col='red') +
  geom_hline(yintercept=-3 , col='red')

```

#REMOVE OUTLIERS AND REFIT
```{r}
which(rstudent(model5) >3 | rstudent(model5)< -3)
outlierindexs = c(10,11,65,69)
grad_df_noOutliers =train_df[-outlierindexs, ]
cbind(nrow(train_df), nrow(grad_df_noOutliers))
model6=lm(data=grad_df_noOutliers, formula = log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score + Research:CGPA  +Research:University.Rating)
summary(model6)

```

#PLOT RESIDUALS WITH NO OUTLIERS
```{r}
plotResiduals(grad_df_noOutliers, model6)
plotNormalResidual(model6)

```

#USE MODEL5 Since removing outliers provided little extra performance
#Cross Validate Model 5-Fold 

```{r}
cv_control = trainControl(method = 'cv', number=5, verboseIter = TRUE)
cv_model = train(log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score + Research:CGPA  +Research:University.Rating  ,data=train_df, method='lm', trControl = cv_control)

cv_model$results


```


#TEST ON TEST DATASET
```{r}
MAPE = function(y_obs, y_pred){mean(abs((y_obs - y_pred)/y_obs))*100}

odds_preds = predict(model6, test_df)
prob_preds = sigmoid(odds_preds)
test_rmse = RMSE(prob_preds,test_df$Chance.of.Admit)
test_mape = MAPE(test_df$Chance.of.Admit, prob_preds)
cbind(test_rmse, test_mape)

```

