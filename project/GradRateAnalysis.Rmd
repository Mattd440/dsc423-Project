---
title: "GraduationRateAnalysis"
author: "MattDiederick"
date: "February 28, 2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Load Librarys - Load File - clean up data
```{r}
  library(ggplot2)
  library(dplyr)
  library(tidyr)
  library(car)
  library(psych)
  library(leaps)
  library(caret)
  library(QuantPsyc)
  library(corrplot)

  # Load File
  df = read.delim('Admission.csv', header = TRUE, sep=',')

  #Drop Serial.No Column
  grad_df = df[,-c(1)]
  df = df[,-c(1)]
  #SetUp For later Use
  cormatrix = as.data.frame(round(cor(grad_df), 3))
  cormatrix = cormatrix %>% mutate(var1 =rownames(cormatrix))
  
  #Cast Qualitative Variables to Factors
 #grad_df$University.Rating = as.factor(grad_df$University.Rating)
  grad_df$Research = as.factor(grad_df$Research)
  #grad_df$SOP = as.factor(grad_df$SOP)
  #grad_df$LOR = as.factor(grad_df$LOR)
  
  #SCALE FEATURES
  #grad_df$GRE.Score = scale(grad_df$GRE.Score, scale=TRUE, center=TRUE)
  #grad_df$TOEFL.Score = scale(grad_df$TOEFL.Score, scale=TRUE, center=TRUE)
  #head(grad_df,10)
  
  
  
  
  
  #TRAIN TEST SPLIT
  inds = sample(1:nrow(grad_df), .8*nrow(grad_df))
  train_df = grad_df[inds,]
  test_df = grad_df[-inds,]
  
#define function to change ODDS back to PROBABilty
#define function to plot residuals
#define function to create normal qq plot

sigmoid = function(x){
  return (1/(1+exp(-x)))
}


plotResiduals = function(data,model){
  ggplot(data=data,aes(x=sigmoid(fitted(model)), y=rstudent(model))) +
    geom_point() +
    geom_hline(yintercept=0, col='red') +
    ggtitle('Fitted values vs standard residuals')
}

plotNormalResidual = function(model){
  qqnorm(rstandard(model))
  qqline(rstandard(model), col='red')
}
  
```
 
 
# Visualize Data

```{r}

ggplot(data=grad_df, aes(x=Chance.of.Admit))+
  geom_histogram(bins=20)+
  geom_vline(xintercept=mean(grad_df$Chance.of.Admit), color='red')+
  ggtitle('Distribution of Admittance Chance')
skew(grad_df$Chance.of.Admit)
ggplot(data=grad_df, aes(x=(log(Chance.of.Admit/(1-Chance.of.Admit)))))+
  geom_histogram(bins=30)+
  geom_vline(xintercept=mean(log(grad_df$Chance.of.Admit/(1-grad_df$Chance.of.Admit))), color='red')+
  ggtitle('Distribution of Admittance Chance After Transformation')
skew(log(grad_df$Chance.of.Admit/(1-grad_df$Chance.of.Admit)))

ggplot(data=grad_df, aes(x=GRE.Score, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE) +
  ggtitle("Admission by GRE Score")

ggplot(data=grad_df, aes(x=TOEFL.Score, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)+
  ggtitle("Admission by TOEFL Score")

ggplot(data=grad_df, aes(x=SOP, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)+
  ggtitle("Admission by SOP Strength")

ggplot(data=grad_df, aes(x=LOR, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)+
  ggtitle("Admission by GRE Score")+
  ggtitle("Admission by LOR Strength")

ggplot(data=grad_df, aes(x=CGPA, y=Chance.of.Admit, col=Research)) +
  geom_point() +
  geom_smooth(method='lm', se=FALSE)+
  ggtitle("Admission by CPGA")

ggplot(data=grad_df, aes(x=as.factor(University.Rating), y=Chance.of.Admit)) +
  geom_boxplot()+
  ggtitle("Admission by University Ratings")



```

# View Correlations 


```{r}
#CREATE CORRELATION HEATMAP
cleanmatrix = gather(data=cormatrix, key='var2', value='val', -var1)

cormatrix

ggplot(cleanmatrix, aes(x=var1, y=var2, fill=val)) + 
 geom_tile(colour='white', size=.25)+
theme(axis.text.x = element_text(angle = 90, vjust = 0.5))+
  scale_colour_gradientn(colours=terrain.colors(10))

M <- cor(df)
#corrplot.mixed(M)
#corrplot.mixed(M, lower='number', upper='color')
corrplot(M, method = "color", 
         type = "upper", order = "hclust", 
         addCoef.col = "white", # Add coefficient of correlation
         tl.col = "darkblue", tl.srt = 45, #Text label color and rotation
         # Combine with significance level
        
         # hide correlation coefficient on the principal diagonal
         diag = FALSE 
         )

```

# FIT FULL MODEL 
* Here we transform Chance.of.Admit using log(x/1-x) that changes it from a probability to log odds. We then use the sigmoid function above to change our fitted values which are log odds back to probability. This ensure our predictions stay between 0 and 1 since we our predicting probability. These functions our used in logistic regression , but we are predicting probability and not a binary variable so we use the linear model with transformations on the response variable.  
```{r}

full_model = lm(data=train_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit)) ~ . )
summary(full_model)

```

#CHECK FOR MULTICOLLINEARITY
```{r}
as.data.frame(vif(full_model))
```

#PLOT RESIDUALS
```{r}
 plotResiduals(train_df, full_model)
 plotNormalResidual(full_model)
```


# Apply Variable Selection
```{r}
base = lm(data=train_df, formula = Chance.of.Admit ~ 1)
#bward = step(full_model, direction='backward', trace=TRUE)
fward = step(base, scope=list(upper=full_model, lower=~1), direction='forward', trace=TRUE)

```



# Fit Model With Selected Variables ADD HIGHORDER 

```{r}


model2 = lm(data=train_df, formula = log(Chance.of.Admit/(1-Chance.of.Admit)) ~CGPA + GRE.Score + LOR + Research+TOEFL.Score +University.Rating)
model2_2 = lm(data=train_df, formula = log(Chance.of.Admit/(1-Chance.of.Admit)) ~CGPA + GRE.Score + LOR + Research+TOEFL.Score +University.Rating + I(CGPA^2) + I(GRE.Score^2) + I(LOR^2) + I(TOEFL.Score^2)+I(University.Rating^2) + I(CGPA^3) + I(University.Rating^3) + I(GRE.Score^3) + I(LOR^3) + I(TOEFL.Score^3))
#model2_3 = lm(data=train_df, formula = log(Chance.of.Admit/(1-Chance.of.Admit)) ~CGPA + GRE.Score + LOR + Research+TOEFL.Score +University.Rating + I(CGPA^2)  +I(University.Rating^2)+ I(CGPA^3) )
summary(model2_3)

CGPA2 = train_df$CGPA **2
CGPA3 = train_df$CGPA **3
University.Rating2 = train_df$University.Rating **2
train_df = cbind(train_df,CGPA2, CGPA3, University.Rating2)

```



# Fit Model With ALL Interaction Terms

```{r}

model3 = lm(data=train_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit))~(CGPA+ GRE.Score + LOR + Research+ University.Rating + CGPA2 + CGPA3 + University.Rating2)^2)
summary(model3)

```



#Fit Model With Research Interaction

```{r}


model4 = lm(data=train_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score+ Research:TOEFL.Score + Research:CGPA + Research:GRE.Score + CGPA2 + CGPA3 + University.Rating2 +Research:LOR+Research:University.Rating +Research:CGPA2 + Research:University.Rating2 + Research:CGPA3)
summary(model4)
```

# Remove Insignificant Features
```{r}
model5= lm(data=train_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score + CGPA2 + CGPA3 + University.Rating2 +Research:University.Rating  )
summary(model5)
```

#PLOT RESIDUALS
```{r}
plotResiduals(train_df, model5)
plotNormalResidual(model5)

```

#CHECK FOR OUTLIERS
```{r}
infl = influence.measures(model5)
summary(infl)
print('*******COOKS*********')
cooks = cooks.distance(model5)
head(sort(desc(cooks)),10)
print('******HAT**********')
hats = hatvalues(model5)
head(sort(desc(hats)),10)
print('*******RSTUDENT*********')
rs = rstudent(model5)
head(sort(desc(abs(rs))), 10)

ggplot(data=train_df, aes(x=hatvalues(model5), y=rstudent(model5)))+
  geom_point() + 
  geom_hline(yintercept = 3, col='red') +
  geom_hline(yintercept=-3 , col='red')

```

#REMOVE OUTLIERS AND REFIT
```{r}
#which(rstudent(model5) >3 | rstudent(model5)< -3)
outlierindexs = c(10,11,66,67,93)
grad_df_noOutliers =train_df[-outlierindexs, ]
cbind(nrow(train_df), nrow(grad_df_noOutliers))
model6=lm(data=grad_df_noOutliers, formula = log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score + CGPA2 + CGPA3 + University.Rating2 +Research:University.Rating )
summary(model6)

```

#PLOT RESIDUALS WITH NO OUTLIERS
```{r}
plotResiduals(grad_df_noOutliers, model6)
plotNormalResidual(model6)

```

#USE MODEL5 Since removing outliers provided little extra performance
#Cross Validate Model 5-Fold 

```{r}
cv_control = trainControl(method = 'cv', number=5, verboseIter = TRUE)
cv_model = train(log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score + CGPA2 + CGPA3 + University.Rating2 +Research:University.Rating ,data=train_df, method='lm', trControl = cv_control)

cv_model$results


```


#TEST ON TEST DATASET
```{r}
MAPE = function(y_obs, y_pred){mean(abs((y_obs - y_pred)/y_obs))*100}

tCGPA3 = test_df$CGPA **3
tUniversity.Rating2 = test_df$University.Rating **2

test_data = data.frame(CGPA=test_df$CGPA, GRE.Score=test_df$GRE.Score, LOR=test_df$LOR, Research=test_df$Research, University.Rating=test_df$University.Rating, TOEFL.Score=test_df$TOEFL.Score, CGPA2=(test_df$CGPA**2) ,CGPA3=(test_df$CGPA**3) ,University.Rating2=(test_df$University.Rating**2))
odds_preds = predict(model5, test_data)
prob_preds = sigmoid(odds_preds)
test_rmse = RMSE(prob_preds,test_df$Chance.of.Admit)
test_mape = MAPE(test_df$Chance.of.Admit, prob_preds)
cbind(test_rmse, test_mape)

model5_test= lm(test_df, formula=log(Chance.of.Admit/(1-Chance.of.Admit))~CGPA+GRE.Score+LOR+Research + University.Rating+ TOEFL.Score + I(CGPA^2) + I(CGPA^3) + I(University.Rating^2) +Research:University.Rating)
summary(model5_test)

```

